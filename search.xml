<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[core dump文件分析和调试]]></title>
    <url>%2Fcore-dump%E6%96%87%E4%BB%B6%E5%88%86%E6%9E%90%E5%92%8C%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[core介绍当程序运行的过程中异常终止或崩溃，操作系统会将程序当时的内存状态记录下来，保存在一个文件中，这种行为就叫做Core Dump（中文有的翻译成“核心转储”)。我们可以认为 core dump 是“内存快照”，但实际上，除了内存信息之外，还有些关键的程序运行状态也会同时 dump 下来，例如寄存器信息（包括程序指针、栈指针等）、内存管理信息、其他处理器和操作系统状态和信息。 core文件生成core文件的生成开关和大小限制 使用ulimit -c命令可查看core文件的生成开关。若结果为0，则表示关闭了此功能，不会生成core文件。 使用ulimit -c命令可查看core文件的生成开关。若结果为0，则表示关闭了此功能，不会生成core文件。 使用ulimit -c filesize命令，可以限制core文件的大小（filesize的单位为kbyte）。若ulimit -c unlimited，则表示core文件的大小不受限制。如果生成的信息超过此大小，将会被裁剪，最终生成一个不完整的core文件。在调试此core文件的时候，gdb会提示错误。 在bash中使用ulimit -c unlimited修改core开关，仅对当前shell生效，若希望永久生效， core文件生成路径和文件名 /proc/sys/kernel/core_uses_pid可以控制产生的core文件的文件名中是否添加pid作为扩展，如果添加则文件内容为1，否则为0 /proc/sys/kernel/core_pattern可以设置格式化的core文件保存位置或文件名，比如原来文件内容是core-%e可以这样修改: 1echo "/corefile/core-%e-%p-%t" &gt; /proc/sys/kernel/core_pattern 将会控制所产生的core文件会存放到/corefile目录下，产生的文件名为core-命令名-pid-时间戳 如果core_uses_pid这个文件的内容被配置成1，那么即使core_pattern中没有设置%p，最后生成的core dump文件名仍会加上进程ID。 注意linux的内核参数信息都存在内存中，因此可以用过命令直接修改，并直接生效。但是当系统reboot后，之前设置的参数值就会丢失，而系统每次启动时都会去/etc/sysctl.conf文件中读取内核参数。 因此可以将参数写在这个配置文件中，如： kernel.core_pattern = %e.core.%p 并保存退出，执行下面指令使其生效 sysctl -p 在我的linux机器中，core_pattern的配置为|/usr/lib/systemd/systemd-coredump %P %u %g %s %t %e， |表示使用管道，将core文件重定向给后边的程序处理，这里是交由systemd-coredump程序处理。要获取core文件则应该使用coredumpctl命令获取core文件(需要sudo权限)。 这里我使用a.out产生一个core文件。首先使用sudo coredumpctl list | grep a.out获取core文件的信息。显示结果如下：1Wed 2018-09-19 17:22:39 CST 54666 1100 0 11 * /polestar_build/home/dev/test/core/a.out 从结果中知道PID是54666,可以使用sudo coredumpctl dump 54666 -o a.dump,将core文件dump到a.dump中。 core文件格式core文件是一个标准的ELF格式的文件，使用readelf工具可以对core文件的属性进行检查。如下所示，core文件的TYPE显示是CORE (Core file)。123456789101112131415161718192021:~/test/core&gt; readelf -h a.dumpELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: CORE (Core file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x0 Start of program headers: 64 (bytes into file) Start of section headers: 0 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 36 Size of section headers: 0 (bytes) Number of section headers: 0 Section header string table index: 0 gdb加载core文件使用gdb加载core文件的命令为gdb -c core_file_name，也可以再启动gdb后使用命令core core_file_name在gdb中随时加载core文件。 在没有加载可执行程序以前，仅仅可以使用gdb对core文件内的内容进行检视，包括寄存器的值，以及内存分页(mappings)，调用栈内存等信息。所以还需要加载可执行程序，结合执行文件和core文件分析。加载方法是在bash命令行使用gdb exe_file,也可以在启动gdb后使用命令file exe_file随时加载文件。 有点可执行文件不包含符号表，加载文件后会提示no debugging symbols found， 如果有符号文件，可以使用命令”file exe_file.sym`读取符号表。 当可执行文件和core文件都加载完成后，就可以对案发现场进行验尸了， 设置sharedlibrary搜索路径如果程序需要的动态链接库在对应的路径，则gdb可以搜索加载到so文件。如果需要加载的so放在其他目录，比如，将所有依赖的so放在了.libs目录下，就需要设置链接库搜索路径。可以使用set sysroot和set solib-search-path这两个命令设置so的搜索路径。需要注意的是这两个命令必须在加载core文件前使用。下边是一个例子。第二行表示so的搜索路径包括/lib,/usr/lib和./三个目录，gdb会尝试在这三个目录中搜索依赖的so文件。12345(gdb) set sysroot /no/such/file(gdb) set solib-search-path /lib:/usr/lib:./ (gdb) file Framwork.out #加载执行程序(gdb) core core-1537131595-162728-14957-10 #加载core文件(gdb) info sharedlibrary #查看已经加载的动态链接库 set sysroot 与 set solib-absolute-prefix 是同一条命令，实际上，set sysroot是set solib-absolute-prefix 的别名。 set solib-search-path设置动态库的搜索路径，该命令可设置多个搜索路径，路径之间使用“:”隔开（在linux中为冒号，DOS和Win32中为分号）。 set solib-absolute-prefix 与 set solib-search-path 的区别：总体上来说solib-absolute-prefix设置库的绝对路径前缀，只对绝对路径有效；而solib-search-path设置库的搜索路径，对绝对路径和相对路径均起作用。（编译器自动链接的so库多采用绝对路径）。 常用命令 使用bt命令可以查看程序core文件现场的调用栈。仍然使用前文中的a.out。 栈帧0明显是对空指针函数进行调用，导致内存访问出错。 123456:~/test/core&gt; gdb a.out -c a.dump(gdb) bt#0 0x0000000000000000 in ?? ()#1 0x000000000040058f in register_tm_clones ()#2 0x00000000004005b0 in register_tm_clones ()#3 0x0000000000000000 in ?? () i r可以查看寄存器，x可以查看内存空间，例如x/20wx $sp查看栈顶的20个WORD。 info proc mappings查看内存映射表。 其他gdb命令参考gdb手册。 在arm程序中，由于压栈的寄存器不一致的问题，可能会导致gdb回溯调用栈失败,如下所示，此时需要结合汇编代码人工对调用栈进行回溯。参考另一篇文章。123(gdb) bt#0 0xf72e12a0 in pthread_rwlock_timedwrlock () from /lib/libpthread-2.22.soBacktrace stopped: previous frame identical to this frame (corrupt stack?) 交叉环境下的core dump例如在Arm平台上执行的程序发生了core dump, 但是希望在x86平台的linux机器上对core文件进行调试, 则需要使用交叉环境的arm-linux-gdb，而不是x86的gdb。有两个选择： 下载gdb源码，编译target为arm平台的arm-linux-gdb。 下载预编译的arm-linux-gdb。这里提供一个网上的预编译好的gcc工具链Linaro Releases. 参考链接 http://www.cnblogs.com/hazir/p/linxu_core_dump.html https://blog.csdn.net/ispeller/article/details/20232089 https://stackoverflow.com/questions/33886913/make-gdb-load-a-shared-library-from-a-specific-path https://blog.csdn.net/_xiao/article/details/23289971]]></content>
  </entry>
  <entry>
    <title><![CDATA[arm平台的调用栈回溯(backtrace)]]></title>
    <url>%2FArm%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%B0%83%E7%94%A8%E6%A0%88%E5%9B%9E%E6%BA%AF%2F</url>
    <content type="text"><![CDATA[介绍arm平台的调用栈与x86平台的调用栈大致相同，稍微有些区别，主要在于栈帧的压栈内容和传参方式不同。在arm平台的不同程序，采用的编译选项不同，程序运行期间的栈帧也会不同。有些工具在对arm的调用栈回溯时，可能会遇到无法回溯的情况。例如gdb在使用bt查看core dump文件调用栈时，有时会出现Backtrace stoped的情况,有可能就是栈空间的压栈顺序导致的。当工具无法回溯时，就需要人工结合汇编代码对栈进行回溯，或者使用unwind进行回溯。 arm栈帧结构 通常情况下，arm的调用栈大致结构与x86相同，都是从高地址向低地址扩张。上图是其中一种内存分布。 pc, lr, sp, fp是处理器的寄存器，其含义如下： pc, program counter，程序计数器。程序当前运行的指令会放入到pc寄存器中 fp, 即frame pointer,帧指针。通常指向一个函数的栈帧底部，表示一个函数栈的开始位置。 sp, stack pointer，栈顶指针。指向当前栈空间的顶部位置，当进行push和pop时会一起移动。 lr, link register。在进行函数调用时，会将函数返回后要执行的下一条指令放入lr中，对应x86架构下的返回地址。 调用栈从高地址向低地址增长，当函数调用时，分别将分别将pc, lr, ip和 fp寄存器压入栈中，然后移动sp指针，为当前程序开辟栈空间。 arm官方手册描述如下： 一个arm程序，在任一时刻都存在十五个通用寄存器，这取决于当前的处理器模式。 它们分别是 r0-r12、sp、lr。sp（或 r13）是堆栈指针。 C 和 C++ 编译器始终将 sp 用作堆栈指针。 在 Thumb-2 中，sp 被严格定义为堆栈指针，因此许多对堆栈操作无用而又使用了 sp 的指令会产生不可预测的结果。 建议您不要将 sp 用作通用寄存器。在用户模式下，lr（或 r14）用作链接寄存器 (lr)，用于存储调用子例程时的返回地址。 如果返回地址存储在堆栈上，则也可将 r14 用作通用寄存器。在异常处理模式下，lr 存放异常的返回地址；如果在一个异常内执行了子例程调用，则 lr 存放子例程的返回地址。如果返回地址存储在堆栈上，则可将 lr 用作通用寄存器。 除了官方手册中描述的sp,lr寄存器，通常r12还会作为fp寄存器。fp寄存器对于程序的运行没有帮助，主要用于对栈帧的回溯。因为sp时刻指向的栈顶，通过fp得知上一个栈帧的起始位置。 上图的调用栈对应的汇编代码如下。 8514行将当前的sp保存在ip中(ip只是个通用寄存器，用来在函数间分析和调用时暂存数据,通常为r12); 8518行将4个寄存器从右向左依次压栈。 851c行将保存的ip减4，得到当前被调用函数的fp地址，即指向栈里的pc位置。 8520行将sp减8，为栈空间开辟出8个字节的大小，用于存放局部便令。 1234567891011121314151600008514 &lt;func1&gt;: 8514: e1a0c00d mov ip, sp 8518: e92dd800 push &#123;fp, ip, lr, pc&#125; 851c: e24cb004 sub fp, ip, #4 8520: e24dd008 sub sp, sp, #8 8524: e3a03000 mov r3, #0 8528: e50b3010 str r3, [fp, #-16] 852c: e30805dc movw r0, #34268 ; 0x85dc 8530: e3400000 movt r0, #0 8534: ebffff9d bl 83b0 &lt;puts@plt&gt; 8538: e51b3010 ldr r3, [fp, #-16] 853c: e12fff33 blx r3 8540: e3a03000 mov r3, #0 8544: e1a00003 mov r0, r3 8548: e24bd00c sub sp, fp, #12 854c: e89da800 ldm sp, &#123;fp, sp, pc&#125; -mapcs-frame编译选项在第一节中，程序压栈的寄存器有{fp, ip, lr, pc} 4个，这是在gcc带有-mapcs-frame的编译选项下编译出来的。而gcc默认情况下的参数为mno-apcs-frame。关于该选项，gcc的手册描述为， Generate a stack frame that is compliant with the ARM Procedure Call Standard for all functions, even if this is not strictly necessary for correct execution of the code. Specifying -fomit-frame-pointer with this option causes the stack frames not to be generated for leaf functions. The default is -mno-apcs-frame. This option is deprecated. 也就是说，该编译选项会产生(push {fp, ip, lr, pc})，保证栈帧的格式。如果没有-mapcs-frame，则不保证帧格式和当前帧格式，GCC生成的指令可能会发生各种变化。在AAPCS发布之后[附录1]，1993年的APCS就已经太旧了，所以在gcc5.0之后，该选项已经被废弃。gcc5.0的更新记录写到： The options -mapcs, -mapcs-frame, -mtpcs-frame and -mtpcs-leaf-frame which are only applicable to the old ABI have been deprecated.至于该参数在将来是否会被gcc移除，那就不知道了。 将第一节中的程序重新使用默认编译选项，用4.7版本的gcc编译，结果如下。这时，fp还在，调用栈push了fp和lr到栈空间，新的fp指向了lr在栈中的位置。 1234567891011121314151617181920212200008514 &lt;func1&gt;: 8514: e92d4800 push &#123;fp, lr&#125; 8518: e28db004 add fp, sp, #4 851c: e24dd008 sub sp, sp, #8 8520: e3a03000 mov r3, #0 8524: e50b3008 str r3, [fp, #-8] 8528: e30805d4 movw r0, #34260 ; 0x85d4 852c: e3400000 movt r0, #0 8530: ebffff9e bl 83b0 &lt;puts@plt&gt; 8534: e51b3008 ldr r3, [fp, #-8] 8538: e12fff33 blx r3 853c: e3a03000 mov r3, #0 8540: e1a00003 mov r0, r3 8544: e24bd004 sub sp, fp, #4 8548: e8bd8800 pop &#123;fp, pc&#125; 0000854c &lt;main&gt;: 854c: e92d4800 push &#123;fp, lr&#125; 8550: e28db004 add fp, sp, #4 8554: ebffffee bl 8514 &lt;func1&gt; 8558: e1a00003 mov r0, r3 855c: e8bd8800 pop &#123;fp, pc&#125; 使用gcc-7.3默认选项编译结果如下，fp已经不在了，虽然这里仍然可能通过r7得知上个栈帧的位置，但是已经没法使用fp获取栈帧了。此时是不保证栈帧保存在栈中的。所以依赖栈帧内容进行恢复已经非常不可靠。那么既然无法依赖fp，那该怎么进行栈帧回溯呢，gnu说使用unwind方法回溯，这节暂时不会介绍unwind方法。 123456789101112131415161718192021222324000103c8 &lt;func1&gt;: 103c8: b580 push &#123;r7, lr&#125; 103ca: b082 sub sp, #8 103cc: af00 add r7, sp, #0 103ce: 2300 movs r3, #0 103d0: 607b str r3, [r7, #4] 103d2: f240 4048 movw r0, #1096 ; 0x448 103d6: f2c0 0001 movt r0, #1 103da: f7ff ef7e blx 102d8 &lt;puts@plt&gt; 103de: 687b ldr r3, [r7, #4] 103e0: 4798 blx r3 103e2: 2300 movs r3, #0 103e4: 4618 mov r0, r3 103e6: 3708 adds r7, #8 103e8: 46bd mov sp, r7 103ea: bd80 pop &#123;r7, pc&#125; 000103ec &lt;main&gt;: 103ec: b580 push &#123;r7, lr&#125; 103ee: af00 add r7, sp, #0 103f0: f7ff ffea bl 103c8 &lt;func1&gt; 103f4: 2300 movs r3, #0 103f6: 4618 mov r0, r3 103f8: bd80 pop &#123;r7, pc&#125; 使用栈帧进行回溯这一节使用gcc4.7版本，默认编译选项编译出来的程序，演示调用栈回溯。该编译选项下，压栈的寄存器为{fp, lr}。 下边的内容是一段core dump中的寄存器和调用栈，本节将对这段内容进行回溯。12345678910111213Reg: r9, Val = 0xf7578000; Reg: r10, Val = 0x00000001; Reg: fp, Val = 0x827d3104; Reg: ip, Val = 0xf7578ae0; Reg: sp, Val = 0x827d30e0; Reg: lr, Val = 0xf7549990; Reg: pc, Val = 0xf7548c20; Reg: cpsr, Val = 0x60000210;0x827d30e0: 0x00000031 0x827d31a0 0x00000001 0xd5dff060 0x827d30f0: 0xd5e0e6b1 0xd5dec134 0xf7578000 0xf7577c40 0x827d3100: 0x827d313c 0xf7549990 0x827d3140: 0x00000000 0xd5dec104 0xf7568514 0x00000002 0x827d3150: 0xd5dec104 0xf7577c40 0xf7577c38 0xd5de9224 0x827d3160: 0x827d31a0 0xf757a084 0xf7577c40 0xd5df6dd4 0x827d3170: 0x827d3194 0x00000001 0xd5e0e678 0xd5dec104 0x827d3180: 0xd5de9224 0xf7568548 0x00000000 0xf7568550 当前sp地址为0x827d30e0，fp地址为0x827d3104，从而得知当前函数frame0的栈帧。fp指向的地址0x827d3104为frame1的lr,0x827d3100为上一个栈帧的fp。 1230x827d30e0: 0x00000031 0x827d31a0 0x00000001 0xd5dff060 0x827d30f0: 0xd5e0e6b1 0xd5dec134 0xf7578000 0xf7577c40 0x827d3100: 0x827d313c(fp) 0xf7549990(lr) 从frame0的fp地址0x827d313c可知，frame1的调用栈起始地址，去掉frame0的内容，得到frame1的栈帧。 1234 0x827d312c 0xf7530c14 0x827d3110: 0xd5dff060 0x0000002c 0xd5e0e6b1 0xd5e0e6b1 0x827d3120: 0x00000001 0xd5e0e6b1 0xd5dff060 0xd5dec134 0x827d3130: 0xf7578000 0xf7577c40 0x827d3194(fp) 0xf754ad0c(lr) 依次类推，依次得到frame2、frame3…的栈帧。 当汇编代码的函数调用使用push {fp, ip, lr, pc}时，则上一个栈帧的fp2在当前栈帧的(fp - #4)位置。栈帧的回溯要结合程序的汇编代码具体分析，有可能程序并不使用fp指针，也有可能栈中根本没有保存fp。 unwind方法回溯TODO 附录1-函数调用标准缩略语 PCS Procedure Call Standard. AAPCS Procedure Call Standard for the ARM Architecture (this standard). APCS ARM Procedure Call Standard (obsolete). TPCS Thumb Procedure Call Standard (obsolete). ATPCS ARM-Thumb Procedure Call Standard (precursor to this standar 参考资料 ARM 体系结构概述 Procedure Call Standard for the ARM® Architecture GCC 5 Release Series]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo博客一键式部署到服务器]]></title>
    <url>%2Fgogs%E5%92%8Cgit-hook%E5%AE%9E%E7%8E%B0hex%E5%8D%9A%E5%AE%A2%E4%B8%80%E9%94%AE%E5%BC%8F%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[hexo博客部署到服务器可以使用git和rsync方法，本文首先接受git方式。通过hexo d命令将public目录推送到git服务器，并触发部署代码到网站目录。 搭建git服务器git服务器可以选择gitlab或者gogs, 本文选择gogs。gogs安装链接 安装完gogs服务器后，配置好git网站git.example.com，并创建博客文件仓库,例如blog_public。 设置hexo的git同步方式在hexo的_config.yml文件中, 配置如下的git部署信息，其中repo字段可以在gogs网页上创建仓库后的页面找到。配置完以后，在本地运行hexo d会将public目录推送到git服务器。 12345deploy: type: git repo: &lt;ssh_name&gt;@git.gitserver:&lt;account&gt;/blog_public.git branch: master message: update blog 创建服务器git仓库为了在服务器上访问blog_public仓库的文件，还需要在服务器的网站目录添加git本地仓库，用来存储生成的网站文件。这个文件夹同时也是网站的目录，通过配置nginx，完成对该目录的访问。 1234mkdir /var/www/blog_publiccd /var/www/blog_publicgit initgit clone ~/repos/test.git 使用githook实现自动部署进入到/gogs-repositories//blog_public.git/hooks文件夹，使用vi post-receive创建一个脚本，当你在本地仓库执行git push后就会触发post-receive（关于Git Hok）。post-receive的内容 12345678910#!/usr/bin/env bashunset GIT_DIRNowPath=`pwd`#部署路径为网站服务器设置的文件目录DeployPath="/var/www/blog_public"cd $DeployPathgit pull origin mastercd $NowPathexit 0 之后在本地使用git deploy命令触发提交后，会自动以上脚本，实现自动部署。]]></content>
  </entry>
  <entry>
    <title><![CDATA[appleid切换美区]]></title>
    <url>%2Fappleid%E5%88%87%E6%8D%A2%E7%BE%8E%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[以下内容摘自V2Ex, 经验证有效 刚刚那个美区 APP 被清空的帖子已经展示了我的智商…为了表示歉意(纠正一下我的智商)，我说一下 Apple ID 切换美区的一个奇淫技巧吧；刚刚测试把室友的切换过去了，此办法在切换过程无需任何支付卡号，目前百分百成功…但是为了巩固账号，我建议还是某宝买个礼品卡冲进去小小的消费一下，下面正题: 首先电脑安装 iTunes 最新版(iTunes 啊，不是 App Store)，我这里操作是 mac，如果 win 同学不好使，那只能去日苹果了(基本不可能)； 登陆你的 Apple ID，依次点击 “账户–&gt;查看我的账户–&gt;切换国家 /地区”； 进入贱贱的切换页面，然后一路确认到达支付信息页面，在最新版的 iTunes 中，支付方式是可以选择 Paypal 的； 点击 “Sign in Paypal”，浏览器会跳到 Paypal 登录页面； 最稳的一波操作是: 不管你登没登录，不用去绑卡，直接点击浏览器左下角的 “返回 iTunes” 按钮(应该是英文的)，此时你会发现 iTunes 刷新页面以后神奇的出现了 “None”… 接下来的操作就是找个美国信息，各种撸，最后别忘了充点钱消费一下 我感觉写完了我的智商好了点…玩原谅帽大作战又可以拿第一了 提醒: 尽快操作，这贴子发完了我估计不久这 Bug 可能会被修复 第 1 条附言 · 157 天前 经过多人验证，如果没有 Paypal 选项，请关闭 家庭共享。 如果出不来，可以试试在连接vpn后再试 不需要登录Paypal, 不需要登录Paypal, 不需要登录Paypal(我的登录了也用不了，要使用PayPal支付对此商家的预核准付款，您的PayPal账户和商家账户必须在同一个国家或地区注册。请返回到商家页面选择其他付款方式。) 美国人信息，搜索美国地址生成器即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Centos7启用BBR]]></title>
    <url>%2Fbbr%2F</url>
    <content type="text"><![CDATA[BBR (Bottleneck Bandwidth and RTT) is a new congestion control algorithm which is contributed to the Linux kernel TCP stack by Google. With BBR in place, a Linux server can get significantly increased throughput and reduced latency for connections. Besides, it’s easy to deploy BBR because this algorithm requires only updates on the sender side, not in the network or on the receiver side. In this article, I will show you how to deploy BBR on a Vultr CentOS 7 KVM server instance. Prerequisites A Vultr CentOS 7 x64 server instance. A sudo user. Step 1: Upgrade the kernel using the ELRepo RPM repositoryIn order to use BBR, you need to upgrade the kernel of your CentOS 7 machine to 4.9.0. You can easily get that done using the ELRepo RPM repository. Before the upgrade, you can take a look at the current kernel: uname -rThis command should output a string which resembles: 3.10.0-514.2.2.el7.x86_64As you see, the current kernel is 3.10.0. Install the ELRepo repo: 12sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgsudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm Install the 4.9.0 kernel using the ELRepo repo: 1sudo yum --enablerepo=elrepo-kernel install kernel-ml -y Confirm the result: 123456789rpm -qa | grep kernelIf the installation is successful, you should see kernel-ml-4.9.0-1.el7.elrepo.x86_64 among the output list:kernel-ml-4.9.0-1.el7.elrepo.x86_64kernel-3.10.0-514.el7.x86_64kernel-tools-libs-3.10.0-514.2.2.el7.x86_64kernel-tools-3.10.0-514.2.2.el7.x86_64kernel-3.10.0-514.2.2.el7.x86_64 Now, you need to enable the 4.9.0 kernel by setting up the default grub2 boot entry. Show all entries in the grub2 menu: sudo egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \’The result should resemble: CentOS Linux 7 Rescue a0cbf86a6ef1416a8812657bb4f2b860 (4.9.0-1.el7.elrepo.x86_64)CentOS Linux (4.9.0-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-514.2.2.el7.x86_64) 7 (Core)CentOS Linux (3.10.0-514.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-bf94f46c6bd04792a6a42c91bae645f7) 7 (Core)Since the line count starts at 0 and the 4.9.0 kernel entry is on the second line, set the default boot entry as 1: sudo grub2-set-default 1Reboot the system: sudo shutdown -r nowWhen the server is back online, log back in and rerun the uname command to confirm that you are using the correct Kernel: uname -rYou should see the result as below: 4.9.0-1.el7.elrepo.x86_64 Step 2: Enable BBRIn order to enable the BBR algorithm, you need to modify the sysctl configuration as follows: echo ‘net.core.default_qdisc=fq’ | sudo tee -a /etc/sysctl.confecho ‘net.ipv4.tcp_congestion_control=bbr’ | sudo tee -a /etc/sysctl.confsudo sysctl -pNow, you can use the following commands to confirm that BBR is enabled: sudo sysctl net.ipv4.tcp_available_congestion_controlThe output should resemble: net.ipv4.tcp_available_congestion_control = bbr cubic renoNext, verify with: sudo sysctl -n net.ipv4.tcp_congestion_controlThe output should be: bbrFinally, check that the kernel module was loaded: lsmod | grep bbrThe output will be similar to: tcp_bbr 16384 0 Step 3 (optional): Test the network performance enhancementIn order to test BBR’s network performance enhancement, you can create a file in the web server directory for download, and then test the download speed from a web browser on your desktop machine. 123456sudo yum install httpd -ysudo systemctl start httpd.servicesudo firewall-cmd --zone=public --permanent --add-service=httpsudo firewall-cmd --reloadcd /var/www/htmlsudo dd if=/dev/zero of=500mb.zip bs=1024k count=500 Finally, visit the URL http://[your-server-IP]/500mb.zip from a web browser on your desktop computer, and then evaluate download speed.]]></content>
  </entry>
  <entry>
    <title><![CDATA[create git service with gog]]></title>
    <url>%2Fcreate-git-service-with-gog%2F</url>
    <content type="text"><![CDATA[使用Gog创建git服务今天发现gog挺好玩的，再加上自己的博客即将从wordpress转为hexo，所以希望有个git仓库保存自己的资源。借此机会，试一下gog的服务。 升级mariadb到10.2遇到的第一个难题就是在安装gog时，遇到Error 1071: Specified key was too long; max key length is 767 bytes。经过查找资料，产生这种错误的原因主要是因为 Because of mysql 5.6 (includes prior versions) InnoDB max index length is 767 bytes, mysql 5.7.7 is up to 3072 bytes.If some varchar column’s length is 255, when the character format is utf-8 needs 2553=765 bytes for index length, It’s OK.But, an utf8mb needs 2554=1020 bytes for index length.写这篇文章的时候gog暂时还没有修复这个bug,最简单的办法是升级mysql, 在centos上即mariadb.过程如下： Once you have your MariaDB.repo entry, add it to a file under /etc/yum.repos.d/. (We suggest something like /etc/yum.repos.d/MariaDB.repo.) 12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.2/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 之后执行yum update以及yum install mariadb-server即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用fail2ban阻止暴力破解ssh]]></title>
    <url>%2Ffail2ban%2F</url>
    <content type="text"><![CDATA[今天晚上在博客服务器上进行操作的时候，突然发现登录后提示本次登录前有几十次的失败登录。查看登录Log（/val/log/secure）,发现有大量失败的ssh登录记录。可以判断有人是在尝试暴力破解ssh。以前曾经使用过fail2ban，这次决定用fail2ban来禁止对方登录。以下是在centos7上的操作。记录下来，以便以后查阅。 第一步 安装fail2ban我所用的centos版本默认安装了软件源epel，如果没有安装，则需要先添加软件源。sudo yum install epel-releas 然后安装fail2ban,并设置开机启动 sudo yum install fail2ban sudo systemctl enable fail2ban 第二步 配置Fail2banFail2ban的默认屏蔽设置位于/etc/fail2ban/jail.conf,不要编辑这个文件，因为每次软件更新，新的文件会覆盖你所做的修改。所以这里需要在相同目录创建jail.local文件，将需要的配置写入本地文件，本地文件会覆盖jail.conf。 下边是一个简单的jail.local文件实例。（一开始我没有添加那行enable = true, 导致fail2ban一直不起作用。） 123456789[DEFAULT]# Ban hosts for one hour:bantime = 3600# Override /etc/fail2ban/jail.d/00-firewalld.conf:banaction = iptables-multiport[sshd]enabled = true 这里的bantime可以设置为-1，表示永久封禁。修改完jail.local后就可以重启fail2ban。 sudo systemctl restart fail2ban配置完成后就可以使用fail2ban-client查看状态。 sudo fail2ban-client status 上图只是显示现在是jail数量和名称，要想查看具体封禁的ip，还需要指定jail名。这是我在使用fail2ban后封禁的恶意ip地址。 `sudo fail2ban-client status sshd Explore Available SettingsThe version of jail.local we defined above is a good start, but you may want to adjust a number of other settings. Open jail.conf, and we’ll examine some of the defaults. If you decide to change any of these values, remember that they should be copied to the appropriate section of jail.local and adjusted there, rather than modified in-place. sudo nano /etc/fail2ban/jail.confDefault Settings for All JailsFirst, scroll through the [DEFAULT] section. ignoreip = 127.0.0.1/8You can adjust the source addresses that Fail2ban ignores by adding a value to the ignoreip parameter. Currently, it is configured not to ban any traffic coming from the local machine. You can include additional addresses to ignore by appending them to the end of the parameter, separated by a space. bantime = 600The bantime parameter sets the length of time that a client will be banned when they have failed to authenticate correctly. This is measured in seconds. By default, this is set to 600 seconds, or 10 minutes. findtime = 600maxretry = 3The next two parameters that you want to pay attention to are findtime and maxretry. These work together to establish the conditions under which a client should be banned. The maxretry variable sets the number of tries a client has to authenticate within a window of time defined by findtime, before being banned. With the default settings, Fail2ban will ban a client that unsuccessfully attempts to log in 3 times within a 10 minute window. destemail = root@localhostsendername = Fail2Banmta = sendmailIf you wish to configure email alerts, you may need to override the destemail, sendername, and mtasettings. The destemail parameter sets the email address that should receive ban messages. The sendername sets the value of the “From” field in the email. The mta parameter configures what mail service will be used to send mail. action = $(action_)sThis parameter configures the action that Fail2ban takes when it wants to institute a ban. The value action_ is defined in the file shortly before this parameter. The default action is to simply configure the firewall to reject traffic from the offending host until the ban time elapses. If you would like to configure email alerts, you can override this value from action_ to action_mw. If you want the email to include the relevant log lines, you can change it to action_mwl. You’ll want to make sure you have the appropriate mail settings configured if you choose to use mail alerts. Settings for Individual JailsAfter [DEFAULT], we’ll encounter sections configuring individual jails for different services. These will typically include a port to be banned and a logpath to monitor for malicious access attempts. For example, the SSH jail we already enabled in jail.local has the following settings: /etc/fail2ban/jail.local[sshd] port = sshlogpath = %(sshd_log)sIn this case, ssh is a pre-defined variable for the standard SSH port, and %(sshd_log)s uses a value defined elsewhere in Fail2ban’s standard configuration (this helps keep jail.conf portable between different operating systems). Another setting you may encounter is the filter that will be used to decide whether a line in a log indicates a failed authentication. The filter value is actually a reference to a file located in the /etc/fail2ban/filter.d directory, with its .conf extension removed. This file contains the regular expressions that determine whether a line in the log is bad. We won’t be covering this file in-depth in this guide, because it is fairly complex and the predefined settings match appropriate lines well. However, you can see what kind of filters are available by looking into that directory: ls /etc/fail2ban/filter.dIf you see a file that looks to be related to a service you are using, you should open it with a text editor. Most of the files are fairly well commented and you should be able to tell what type of condition the script was designed to guard against. Most of these filters have appropriate (disabled) sections in jail.conf that we can enable in jail.local if desired. For instance, pretend that we are serving a website using Nginx and realize that a password-protected portion of our site is getting slammed with login attempts. We can tell Fail2ban to use the nginx-http-auth.conf file to check for this condition within the /var/log/nginx/error.log file. This is actually already set up in a section called [nginx-http-auth] in our /etc/fail2ban/jail.conf file. We would just need to add an enabled parameter for the nginx-http-auth jail to jail.local: 12345678910111213/etc/fail2ban/jail.local[DEFAULT]# Ban hosts for one hour:bantime = 3600# Override /etc/fail2ban/jail.d/00-firewalld.conf:banaction = iptables-multiport[sshd]enabled = true[nginx-http-auth]enabled = true And restart the fail2ban service: sudo systemctl restart fail2banMonitor Fail2ban Logs and Firewall ConfigurationIt’s important to know that a service like Fail2ban is working as-intended. Start by using systemctl to check the status of the service: sudo systemctl status fail2banIf something seems amiss here, you can troubleshoot by checking logs for the fail2ban unit since the last boot: sudo journalctl -b -u fail2banNext, use fail2ban-client to query the overall status of fail2ban-server, or any individual jail: sudo fail2ban-client statussudo fail2ban-client status jail_nameFollow Fail2ban’s log for a record of recent actions (press Ctrl-C to exit): sudo tail -F /var/log/fail2ban.logList the current rules configured for iptables: sudo iptables -LShow iptables rules in a format that reflects the commands necessary to enable each rule: sudo iptables -S]]></content>
  </entry>
  <entry>
    <title><![CDATA[破解py2exe打包的程序]]></title>
    <url>%2F%E7%A0%B4%E8%A7%A3py2exe%E6%89%93%E5%8C%85%E7%9A%84%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[py2exe是一个将python脚本转换成windows上的可独立执行的可执行程序(*.exe)的工具，这样，你就可以不用装python而在windows系统上运行这个可执行程序。然而如果不对代码做任何混淆，仅使用Py2exe将程序打包的程序容易被破解。本文讲解破解Py2exe打包的程序的过程。 Py2exe打包的程序的结构Py2exe打包的exe程序有三部分组成，分别是PYTHONxx.dll(xx是版本号，比如27), PYTHONSCRIPT, library.zip。PYTHON27.dll应该就是python的运行环境了。PYTHONSCRIPT是程序开始执行的入口，就想main函数。libarary.zip中是程序用到的库文件，包括自带的库和用户自己写的库文件。 破解程序的时候需要处理的文件就是PYTHONSCRIPT和library.zip。 如果直接使用winrar解压缩exe的pe文件，只能得到library.zip中的文件，如果想要得到全部的文件，需要对处理pe文件。幸运的是，现在已经有程序可以做这个工作： py2exe binary editor。 使用Py2exe Bynary Editor(以下简称pbe) 可以简单的将Py2exe打包的程序dump成原来的三个文件，也可以将修改后的文件再打包回去。使用界面如图。 逆向library.zip中的文件解压缩library.zip中的文件，可以看到有很多的pyo文件。这是python脚本优化过的字节码。不能直接阅读，需要先反编译成py文件。反编译python字节码的工具有很多，我这里使用uncompyle2，uncompile2可以反编译python2.5 到 2.7的文件，如果你需要反编译其他版本的文件，需要使用其他工具。使用uncompule2 可以很简单的将pyo文件反编译成原来的文件。从效果上看，几乎与看源代码无异。这里将不再对uncompyle2的安装和使用进行介绍。具体操作可以看官方文档，或者我下次更新文章时会写出来。 将pyo文件反编译后，找到需要修改的代码，在文件中修改。然后再使用如下命令将修改后的py文件编译成pyo文件：1python -O -m py_compile filename.py 然后用修改后的pyo文件替换library.zip中的文件的源文件。最后，使用pbe仍打开pe文件，最好勾选update options下的Backup Original. 然后点击下边的library，在弹出的窗口中选择更新后的library.zip。然后pbe就会使用更改后的library更新原来的pe文件，而原文件的备份保存为后缀为bak的文件。 逆向PYTHONSCRIPT文件PYTHONSCRIPT文件处理起来相对更麻烦一些。通过阅读py2exe的源码可以了解到该文件的结构, 下边截取了build_exe.py文件里的一部分代码。不关心内容的可以直接跳到逆向的处理过程。12345678910111213141516171819202122232425262728293031323334 # We create a list of code objects, and write it as a marshaled # stream. The framework code then just exec's these in order. # First is our common boot script. boot = self.get_boot_script("common") boot_code = compile(file(boot, "U").read(), os.path.abspath(boot), "exec") code_objects = [boot_code] if self.bundle_files &lt; 3: code_objects.append( compile("import zipextimporter; zipextimporter.install()", "&lt;install zipextimporter&gt;", "exec")) for var_name, var_val in vars.items(): code_objects.append( compile("%s=%r\n" % (var_name, var_val), var_name, "exec") ) if self.custom_boot_script: code_object = compile(file(self.custom_boot_script, "U").read() + "\n", os.path.abspath(self.custom_boot_script), "exec") code_objects.append(code_object) if script: code_object = compile(open(script, "U").read() + "\n", os.path.basename(script), "exec") code_objects.append(code_object) code_bytes = marshal.dumps(code_objects) if self.distribution.zipfile is None: relative_arcname = "" si = struct.pack("iiii", 0x78563412, # a magic value, self.optimize, self.unbuffered, len(code_bytes), ) + relative_arcname + "\000"script_bytes = si + code_bytes + '\000\000'self.announce("add script resource, %d bytes" % len(script_bytes)) 倒数第二行的script_bytes就是最后写入PYTHONSCRIPT文件中的内容。code_bytes是个marshal处理过的list, list中有我们关心的代码。si是一个4个整形大小的结构。 处理过程首先，使用二进制编辑软件将头部的4个整数si和最后的两个0字节去掉，这里使用winhex的示例如图，将阴影部分的内容删去。 将得到的文件另存为一个新的文件，这样就只剩下了 code_bytes。 在 Python 中输入： 12&gt;&gt;&gt;import marshal&gt;&gt;&gt;mylist=marshal.load(open("dumpfile", "r")) #目的是为了把 dump 下来的文件加载到内存当中，成为 Python 的一个对象。 注1：加载dump下来的对象，Python 版本一定要和 dump 时候的版本兼容才行。注2：如果出现EOFError: EOF read where object expected错误，更换到linux操作系统可能可以解决。 12&gt;&gt;&gt;mylist[&lt;code object &lt;module&gt; at 0xb7473ad0, file "C:\Python27\lib\site-packages\py2exe\boot_common.py", line 44&gt;, &lt;code object &lt;module&gt; at 0xb7473b18, file "&lt;install zipextimporter&gt;", line 1&gt;, &lt;code object &lt;module&gt; at 0xb7202a88, file "main.py", line 3&gt;] #包含了 3 个 code object 对象。第一个是 py2exe 初始化用的，第二个是解压 zip 用的，第三个就是我们的关键脚本了。 为了反编译主函数的代码，我们将main.py的code object保存到文件中，然后仍然使用umcompyle2反编译。 1&gt;&gt;&gt;&gt;&gt;&gt; marshal.dump(mylist[2], open("main.pyo","w")) # 将main函数的内容dump到pyo文件中 仅仅将code object dump到文件中还不行，还需要再文件头部添加文件头。用 WinHex 加上 8 个字节的 file header。前 4 个字节代表 Python 版本号，后 4 个字节是 timestamp，可以打开另外一个pyo文件将前 8 个字节复制过去(图中阴影部分)。 然后使用uncompyle2反编译修改后的文件，得到源码。修改过代码之后，按照和本节相反的顺序，打包到源文件中去。 结语将py2exe打包的pe程序反编译回python代码后，如果未经混淆，可以看到几乎与源码无异。Python 开发的商业软件，其安全性还值得商榷。抵御攻击的做法是使用第三方库编译成 native code，使用代码混淆器，或者修改 Python 源代码防止被反汇编。 参考资料： http://bbs.pediy.com/archive/index.php?t-111428.html https://github.com/wibiti/uncompyle2]]></content>
      <categories>
        <category>diasembel</category>
      </categories>
      <tags>
        <tag>py2exe</tag>
        <tag>discompyle</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hashcat破解office(97-03,2007,2010,2013)密码]]></title>
    <url>%2Fhashcat%E7%A0%B4%E8%A7%A3office%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[hashcat 是什么hashcat是一款免费的密码破解工具。它支持使用AMD和NVidia显卡运算，同时也保留使用cpu计算的版本。hashcat号称是世界上最快的密码破解工具，同时也是世界上第一款和唯一一款使用通用图形处理器的工具。它支持windows和linux操作系统，同时支持OpenCL和CUDA。hashcat可以支持破解包括MD5,sha1等150多种加密算法生成的密码。具体特性和支持的加密算法见http://hashcat.net/oclhashcat/。 使用hashcat破解office密码的时候需要指定加密方式，Office 03 - 2013一共有9种hash方式，分别如下 123456789Office 97-03(MD5+RC4,oldoffice$0,oldoffice$1): flag -m 9700Office 97-03(MD5+RC4,collider-mode#1): flag -m 9710Office 97-03(MD5+RC4,collider-mode#2): flag -m 9720Office 97-03(SHA1+RC4,oldoffice$3,oldoffice$4): flag -m 9800Office 97-03(SHA1+RC4,collider-mode#1): flag -m 9810Office 97-03(SHA1+RC4,collider-mode#2): flag -m 9820Office 2007: flag -m 9400Office 2010: flag -m 9500Office 2013: flag -m 9600 本文将介绍如何使用hashcat破解office密码。现在我有一个密码加密的xls文件test.xls，我将在windows平台使用hashcat破解它的密码。 获取文件hash值首先，我们需要获取文件的hash值，使用office2john.py可以提取文件的hash值。运行这个脚本首先需要安装python2.7环境。 在命令行中执行 1$ python office2john.py test.xls 我们这里的例子得到的hash结果为： 123test.xls:$oldoffice$0*2031e09b28a2a13a891dff99bae0927d*65264ab10fdde25b22146e2c94778c50*981ec9419aa5dd88c0340491ed07f38c:::936 111 730895 936 11112798626794 12798538993 12932656957 Microsoft Excel 1::test.xls 第一个:号前边是文件名，紧跟着的是加密方式，这里是oldoffice$0。然后在hashcat官网的wiki页面的options中 Generic hash types 找到对应的代号，这里oldoffice$0的代号是9700。我们用hashcat只需要第一个冒号和第二个冒号之间的内容。 这里是 1$oldoffice$0*2031e09b28a2a13a891dff99bae0927d*65264ab10fdde25b22146e2c94778c50*981ec9419aa5dd88c0340491ed07f38c 将得到的hash值保存到文本文件中，这里保存到hash.txt里。 使用hashcat破解hashcat可以使用字典或者掩码对密码进行攻击。一开始我使用掩码，但是使用较短位数密码时它不断警告字典空间过小，无法发挥GPU的并行运算能力。所以我这里使用了字典。因为这个文件是我同学交给我让我帮忙找回密码的，我猜测密码可能是纯数字的，并且有可能在8位以下。我首先生成了一个4到8位数字密码的字典。 在hashacat目录下打开命令行，输入下面的命令开始破解。1cudaHashcat64.exe -a 0 -m 9700 -o found.txt hash.txt F:\wordlist\wordlist cudaHashcat64.exe 是在英伟达显卡电脑上要运行的版本。 -a 参数指定攻击方式,这里的 0 表示使用字典攻击。 -m 参数指定密码加密方式，这里使用我们查到的9700，代表我们的excel2003加密。 -o 指定结果输出的位置，这里指定输出到found.txt。接下来hash.txt是保存hash值的文件名，F:\wordlist\wordlist是字典的位置和文件名。 程序开始运行后按s可以查看破解状态,按p暂停，按r继续，按q退出。 12345678910111213141516[s]tatus [p]ause [r]esume [b]ypass [q]uit =&gt;Session.Name...: cudaHashcatStatus.........: RunningInput.Mode.....: File (F:\wordlist\wordlist)Hash.Target....: $oldoffice$0*2031e09b28a2a13a891dff99bae0...Hash.Type......: MS Office &lt;= 2003 MD5 + RC4, oldoffice$0, oldoffice$1Time.Started...: Mon Sep 14 00:18:38 2015 (1 sec)Time.Estimated.: Mon Sep 14 00:20:50 2015 (2 mins, 10 secs)Speed.GPU.#1...: 910.1 kH/sRecovered......: 0/1 (0.00%) Digests, 0/1 (0.00%) SaltsProgress.......: 1401856/111110000 (1.26%)Rejected.......: 0/1401856 (0.00%)Restore.Point..: 1401856/111110000 (1.26%)HWMon.GPU.#1...: 65% Util, 54c Temp, N/A Fan 在经过35秒的计算后，文件的密码被算出。屏幕上输出状态，程序结束运行。结果将输出到found.txt中。 123456789101112Session.Name...: cudaHashcatStatus.........: CrackedInput.Mode.....: File (F:\wordlist\wordlist)Hash.Target....: $oldoffice$0*2031e09b28a2a13a891dff99bae0...Hash.Type......: MS Office &lt;= 2003 MD5 + RC4, oldoffice$0, oldoffice$1Time.Started...: Mon Sep 14 00:18:38 2015 (35 secs)Speed.GPU.#1...: 887.4 kH/sRecovered......: 1/1 (100.00%) Digests, 1/1 (100.00%) SaltsProgress.......: 30941184/111110000 (27.85%)Rejected.......: 0/30941184 (0.00%)Restore.Point..: 30940160/111110000 (27.85%)HWMon.GPU.#1...: 27% Util, 58c Temp, N/A Fan 打开found.txt，其中内容如下。1$oldoffice$0*2031e09b28a2a13a891dff99bae0927d*65264ab10fdde25b22146e2c94778c50*981ec9419aa5dd88c0340491ed07f38c:19830312 冒号后边就是破解的密码19830312。至此，这个密码加密的excel的密码就被破解了。 这里使用的excel文件是office2003版本，相对与office2007以后的版本加密算法要弱。所以hashcat运算速度也相对较快。我在使用word2013文件进行测试的时候，hashcat破解速度极慢。而且这个文件的密码是8位纯数字，字典空间较小，所以才能很快的破解出来。]]></content>
      <categories>
        <category>hashcat</category>
      </categories>
      <tags>
        <tag>hashcat</tag>
        <tag>破解密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meteor使用现有的mongodb数据库]]></title>
    <url>%2Fmeteorshi-yong-xian-you-de-mongodbshu-ju-ku%2F</url>
    <content type="text"><![CDATA[meteor使用环境变量MONGO_URL确定mongodb的位置。如果已经存在一个监听在27017端口上的mongdb，则可以使用下边的一句话使用现有的mongodb。1export MONGO_URL=mongodb://localhost:27017/your_db 将其中的your_db替换成你要使用的数据库。每次重开一个终端或主机重启的时候，环境变量会还原，需要重新导入。]]></content>
  </entry>
  <entry>
    <title><![CDATA[字母转换大小写]]></title>
    <url>%2Fzi-mu-zhuan-huan-da-xiao-xie%2F</url>
    <content type="text"><![CDATA[在ASCII码中，大写字母A-Z的编码是41h-5Ah,小写字母a-z的编码是61h-8Ah,中间相差20h，用二进制表示就是0010 0000。所以当从大写转换成小写或者从小写转换到大写的时候只需改变一位即可。可以用位操作很快的实现。 c语言示例代码1234567891011121314//小写转换成大写char upper(char input)&#123; return input &amp; 0xdf;&#125;//大写转换成小写char lower(char input)&#123; return input | 0x20 ;&#125;//大小写转换,大写变小写，小写变大写。char reverse(char input)&#123; return input ^ 0x20;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Vim tutorial]]></title>
    <url>%2Fvim-tutorial%2F</url>
    <content type="text"><![CDATA[写在前边请忽略任何将VIM配置成类似Notepad编辑器的建议（Please ignore anyone who provides advice on how to configure Vim to operate like Notepad!） 编辑Vim中有Normal(普通)，Insert（编辑）和Visual（视图），在Insert模式下进行编辑。从普通模式进入编辑模式的快捷键： i 在光标当前位置插入 a 在光标当前位置之后插入 I 在当前行起始位置插入 A 在当前行末尾插入 o 在当前行下方插入新的行 O 在当前行上方插入一行 s 删除当前位置的字符并在此插入 S 删除当前行并进入编辑模式 cw 替换当前位置的单词并进入编辑 (any movement command can be substituted for w) cc 同S (change line) C 删除从光标位置到行尾的单词并进入编辑模式 保存并退出在普通模式下使用 :q退出不保存，:w保存，:w &lt;filename&gt;保存问,:wq或:x保存并退出 :wa write all changed files (save all changes) :xa exit all (save all changes and close Vim) :qa quit all (close Vim, but not if there are unsaved changes) :qa! quit all (close Vim without saving—discard any changes) reference: http://vim.wikia.com/wiki/Tutorial]]></content>
  </entry>
  <entry>
    <title><![CDATA[Vim 学习笔记-使用tab分页（1）]]></title>
    <url>%2Fvim-xue-xi-bi-ji-shi-yong-biao-qian-lan%2F</url>
    <content type="text"><![CDATA[使用tab分页下边的的内容会介绍利用tab分页打开和编辑文件。在Vim中，所有文本都被放在缓存区，每一个缓存区都可以被显示在任意数量的窗口或任意数量的tab页面中（就像谷歌浏览器的标签页，以下将用tab描述）。 在很多编辑器中（不包括Vim）,一个文件只能在一个分页中打开，而每一个分页只能显示一个文件，一个文件不能出现在多个分页中。在Vim中并没有这个限制。Vim中的tabs可以很方便的组织你的工作。 (本文将省略Gvim的说明) 打开和关闭tabs当打开Vim的时候，可以指定-p参数，使得每个文件在各自的tab中显示（直到设定的上限tabpagemax）。例如：12vim -p first.txt second.txtgvim -p *.txt 当Vim被启动后，可以使用下边的命令创建和关闭文件12345:tabedit &#123;file&#125; #在一个新的tab中编辑file文件:tabfind &#123;file&#125; #在‘path'变量中搜索指定文件file,并在新的tab中打开:tabclose #关闭当前的tab:tabclose &#123;i&#125; #关闭第i个tab:tabonly #关闭除当前tab外的所有tab ==:tabfind== 命令会使用Vim中的==’path’==选项最顶搜索的路径。例如下边的例子中。==.==会使Vim搜索当前文件所在的目录，然后搜索当前目录（两个连续的逗号==,,==），接着搜索当前目录的子目录(==**==)1:set path=.,,** 记住，就像Vim中的很多命令一样，你只要输入足够多的字符使Vim分辨出来你的命令就可以了。例如，你可以用==:tabe==和==:tabf==来代替==:tabedit==和==:tabfind== 另外，由于Vim中有大量的关于窗口的操作命令，在这些命令前加上tab就可以是这些命令变成对tab的操作。例如：1234:tab ball #show each buffer in a tab (up to 'tabpagemax' tabs) （我现在也不是很懂）:tab help #在它自己的tab页打开help文件:tab drop &#123;file&#125; #为file打开一个新的tab, 如果已经存在，就跳转到它所在的窗口或tab:tab split #将当前文件完全复制到另外一个新的tab中 使用==:sp myfile.txt==类似的命令可以再当前的tab中打开一个新的窗口(即将当前页面分裂成两个窗口)。可以使用命令==Ctrl-w T==将那个窗口移动到一个新的tab, 也可以在不改变原来页面的情况下，使用==:tab sp==将那个窗口复制的另外一个页面。 你可以使用==Ctrl-W c==关闭当前窗口，如果这个窗口已经是当前tab的最后一个窗口，这个tab也会被关闭。 如果你现在编辑的文件中含有另外一个文件的名字，你可以将光标移动文江名上，然后按==gf==去编辑那个文件。使用==Ctrl-W gf==会让该文件显示在一个新的tab中。 导航123456789:tabs #显示所有的tab，包括它们中的窗口:tabm 0 #将当前tab移动到第一个:tabm #将当前tab移动到最后一个:tabm &#123;i&#125; #将当前tab移动到第i+1个:tabn #切换到下个tab:tabp #切换到上个tab:tabfirst #切换到第一个tab:tablast #切换到最后一个tab 在常规模式(命令视图)中，可以使用这些快捷键：123gt #切换到下个tabgT #切换到上个tab&#123;i&#125;gt #切换到第i个tab 记住gt命令是从1开始计数。0gt和1gt都会跳到第一个tab。 捷径下边这些内容需要添加到vimrc文件中，可以得到一些关于tab操作的快捷键。（我省略了暂时没看） With the following mappings (which require gvim), you can press Ctrl-Left or Ctrl-Right to go to the previous or next tabs, and can press Alt-Left or Alt-Right to move the current tab to the left or right.1234nnoremap &lt;C-Left&gt; :tabprevious&lt;CR&gt;nnoremap &lt;C-Right&gt; :tabnext&lt;CR&gt;nnoremap &lt;silent&gt; &lt;A-Left&gt; :execute 'silent! tabmove ' . (tabpagenr()-2)&lt;CR&gt;nnoremap &lt;silent&gt; &lt;A-Right&gt; :execute 'silent! tabmove ' . tabpagenr()&lt;CR&gt; With the following, you can press F8 to show all buffers in tabs, or to close all tabs (toggle: it alternately executes :tab ball and :tabo).12let notabs = 0nnoremap &lt;silent&gt; &lt;F8&gt; :let notabs=!notabs&lt;Bar&gt;:if notabs&lt;Bar&gt;:tabo&lt;Bar&gt;:else&lt;Bar&gt;:tab ball&lt;Bar&gt;:tabn&lt;Bar&gt;:endif&lt;CR&gt; The following command abbreviation allows typing :tabv myfile.txt to view the specified file in a new tab; the buffer is read-only and nomodifiable so you cannot accidentally change it.1cabbrev tabv tab sview +setlocal\ nomodifiable Refrence:http://vim.wikia.com/wiki/Using_tab_pages]]></content>
      <tags>
        <tag>
- vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 学习笔记-复制和粘贴]]></title>
    <url>%2Fvim-xue-xi-bi-ji-1%2F</url>
    <content type="text"><![CDATA[最近随着在linux下工作的时间越来越多，Vim的使用次数也越来在越多。以前曾经掌握的基本操作本来就不多，后来又忘掉了很多，现在越来越感觉会的快捷键不够用了，导致编辑效率不高。所以现在每天花点时间重新学些Vim的东西吧。 复制和粘贴剪切（复制）和粘贴 将光标移动到剪切的开头部分 按v开始选择字符（按住V选择整行） 将光标移动到希望剪切部分的尾部。 按d剪切（按y复制） 移动到希望粘贴的地方 按P粘贴到光标前，按p粘贴到光标之后 tips: d = delete = cut y = yank = copy 多段粘贴当删除或者复制一段文字的时候，这段文字被保存在Vim的未命名寄存器中。Vim中还可以使用用单个字母标记的命名寄存器。用一个双引号加一个字母指定要使用的寄存器。 例如，选择==hello==后，按==”ay==将”hello”保存到==a==寄存器，然后选择==world==,按==”by==保存到==b==寄存器。当要粘贴的时候，分别使用==”ap==和==”bp==粘贴”hello”和”world”。当然也可以使用==”aP==和==”bP==来粘贴到光标之前。 在不同Vim窗口（或终端）中粘贴在复制文本后，为了复制到另一个文件，先打开一个缓冲区：1:e ~/dummy 将文本粘贴到缓冲区 保存缓冲区的内容(:w) 切换到前一个缓冲区以释放交换文件*.swp 现在可以换到另外一个窗口或者终端 将光标移动到希望粘贴的地方 读取dummy文件(:r ~/dummy) 增加缓冲区大小在Vim中有时候最多只能复制50行。可以通过增加缓存区大小来解除这个限制：123456:help 'viminfo'...&lt; Maximum number of lines saved for each register....:set viminfo?:set viminfo='100,&lt;100,s10,h]]></content>
      <tags>
        <tag>
- vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python制作一个简单爬虫下载500px的图片]]></title>
    <url>%2Ffen-xi-500pxde-apibing-qie-zhi-zuo-ge-pa-chong%2F</url>
    <content type="text"><![CDATA[500px.com是一个专业的摄影图片社区，在500px上可以发现和分享高质量的图片，它的Logo是“最出色的摄影社区”。我在他的网站上找到很多喜爱的图片。但是，当你发现自己喜爱的图片然后右键的时候，糟糕，提示“copyright blabla”,反正就是不让下载。今天，我在这里记录我分析他的API，并且用python自动抓取图片的过程。 获得图片500px阻止了右键下载的功能，当然，突破这个限制很简单，只要在chrome浏览器里开启调试模式，然后在Elements里搜索”jpg”，就可以找到下载地址。它的下载地址是这样一个形式的。可以很轻松的找到需要的下载地址。 获得一张图片的地址，还不够，我还想写个脚本自动抓取它的图片，本来以为像平常一样简单，但是真正去抓的时候，却遇到点麻烦。 动态网站？怎么抓我在获得popular页（popular页是所有图片的开始）的数据之后，使用正则表达式搜索其中的链接，奇怪是没有找到任何图片链接，因为我学python做爬虫也没多久，所以我并不知道是怎么回事。我把抓过来的页面写到文件里，然后用编辑器打开。突然恍然大悟。推荐页面的图片都是动态加载的。必须在浏览器里运行javascript后才能得到目标页面。但是我的爬虫过于简陋显然是没办法运行javascript的，也就谈不上抓取真正想要数据。 现在怎么办？动态页面？我没有办法，只好向室友请教（室友是个经验丰富的大牛）。他告诉我，我需要webkit。 嗯。又学到了一个新东西。他就帮我google，发现了pyv8。让我去看看。 pyv8，竟然还有python的V8引擎，好东西。可是我不会用。。。我这会有搜索了一下其他的webkit。知道了没有界面的headless webkit是我想要的东西，其中PhantomJS 是一个非常好的webkit。但是我要用python，PhantomJs 是javascript的，我看到一些在python里使用PhantomJs的方法，也尝试了一下。但是在安装阶段，卧槽，莫名其妙的错误（写博客的时候已经忘了当时什么错误了，反正玩linux经常回出现各种莫名其妙的错误）。blalba， 在webkit这块折腾了许久，没搞定，其中还发现了Ghost这么一个包，可以直接来抓动态网页，好厉害，可是我用的python3，在我机器上没有安装成功。 一顿折腾，我打算放弃的时候，想到在知乎上好像看到一个方法“找到网站的API,直接获取数据”。接下来就是进入分析阶段了。 获得json数据首先还要进入浏览器的调试模式，进入500px的推荐页面。看到过程是下边这样的。 可以看到，一开始获得“没有价值的”首页，然后出现一个ping ，看到ping的地址，就马上知道这就是我们的目标了。记下来都是获得的各种css和js文件。继续往下看，发现在开始获取图片资源以前又get了一次api请求。 这时候点开这个请求，发现是个json数据，经过观察，就是首页推荐里的所有图片的信息。 这时候的目标就很简单了，看给Api发出的请求，然后伪造请求。 在试了几次之后，发现其中的auth-token是很重要的，否则就请求一定会被拒绝。但是auth-token从哪里来，cookie里没有，找啊找，最后发现auth-token就在首页的head区域中。 接下来的过程自然就是获取推荐页，获得auth-token，然后伪造请求向api发数据（我还模仿浏览器先ping了一下），从json中获得图片的下载链接。在处理json时，用Python3自带的json处理包可以很方便的处理数据。这里有意思的是，它发回的json包里的下载地址都是4.jpg结尾，但是把4改成5之后，获得的图片就比较更清晰（实际上因为高清原图都要买，我还没法弄到）。 由于学python还没多久，所以编程上还不是很好，后来想将程序改成一个class，也遇到困难，干脆不做了，毕竟只是一个用用就不用的脚本，不是当做产品的。 我的代码： # a python program to grap 500pk and download pictures # author: Aaron # 3 Feb 2014 import urllib.request import urllib.parse import re import http.cookiejar import time import json import pickle import os import os.path import queue BASE_URL = &apos;https://500px.com/&apos; START_URL = &apos;https://500px.com/popular&apos; #API = &quot;https://api.500px.com/v1/photos?rpp=38&amp;feature=popular&amp;image_size%5B%5D=3&amp;image_size%5B%5D=4&amp;\ #page=&quot;+page+&quot;&amp;sort=&amp;include_states=true&amp;formats=jpeg%2Clytro&amp;only=&amp;authenticity_token=&quot; API_BASE = &quot;https://api.500px.com/v1/photos?&quot; Cauth_token = &apos;A%2Be7bNn8RWMGBFySgzumEk3AZhzqXQ0JPs3zUScmm0%3D&apos; API_PING = &apos;https://api.500px.com/v1/ping&apos; SAVE_PATH = &apos;./image/&apos; query = {&apos;rpp&apos;: [&apos;38&apos;], &apos;feature&apos;: [&apos;popular&apos;], &apos;image_size[]&apos;: [&apos;3&apos;, &apos;4&apos;], &apos;page&apos;: [&apos;1&apos;], &apos;sort&apos;: [&apos;&apos;], &apos;include_states&apos;: [&apos;true&apos;], &apos;formats&apos;: [&apos;jpeg,lytro&apos;], &apos;only&apos;: [&apos;&apos;], #category, leave blank for all. &apos;authenticity_token&apos;: [&apos;&apos;] } start = 1 end = 10 visited = [] wait_list = queue.Queue() #build opener cj = http.cookiejar.CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) opener.addheaders = [(&apos;User-Agent&apos;,&apos; Mozilla/5.0 (Windows NT 6.3; Win64; x64)\ AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36&apos;)] def get_token(taget_url): with opener.open(taget_url) as f: data = f.read().decode() #print(data) pattern = &apos;&lt;a data-bind=&quot;photo_link&quot; data-ga-action=&quot;Image&quot; data-ga-category=&quot;Photo Thumbnail&quot; href=&quot;\S*&quot; id=&quot;\S*&quot;&gt;&apos; test_pat = &apos;href=&quot;\S*&quot;&apos; test_data = re.search(test_pat,data) print(test_data.group()) pic_reg = re.compile(pattern) res = pic_reg.search(data) print(res) auth_patt = &apos;content=&quot;(?P&lt;auth_token&gt;\S*)&quot;.name=&quot;csrf-token&quot;&apos; auth = re.search(auth_patt,data) print(&apos;auth&apos;,auth.group(&apos;auth_token&apos;)) auth_token = auth.group(&apos;auth_token&apos;) save_file = open(&apos;test.html&apos;,&apos;w+&apos;) save_file.write(data) save_file.close() return auth_token def save_image(image): pass def get_image(photo): &apos;&apos;&apos; get and save the image &apos;&apos;&apos; image_url = photo[&apos;image_url&apos;][1] image_id = photo[&apos;id&apos;] image_category = photo[&apos;category&apos;] path = SAVE_PATH + str(image_category)+&apos;/&apos; if not os.path.exists(path): os.mkdir(path) with opener.open(image_url) as res: image = res.read() with open(path+str(image_id)+&apos;.jpg&apos;,&apos;wb&apos;) as file: file.write(image) file.close() #print(photo_id,&apos;is ok&apos;) def ping(): opener.addheaders = [(&apos;Origin&apos;,&apos;https://500px.com&apos;),(&apos;Referer&apos;,&apos;https://500px.com/popular&apos;)] opener.open(API_PING) def get_list(API): global wait_list global visited print(&quot;reading list&quot;) with opener.open(API) as f: json_data = f.read().decode() pic_list = json.loads(json_data) for photo in pic_list[&apos;photos&apos;]: if photo[&apos;id&apos;] not in visited: photo[&apos;image_url&apos;][1] = photo[&apos;image_url&apos;][1].replace(&apos;4.jpg&apos;,&apos;5.jpg&apos;) wait_list.put(photo) print(&apos;dowloaded:&apos;,len(visited)) print(&apos;to be dowloaded:&apos;,wait_list.qsize()) json_file = open(&apos;pics.json&apos;,&apos;w+&apos;) json_file.write(json_data) json_file.close() def handle_wait_list(page): total = wait_list.qsize() count = 0 while not wait_list.empty(): photo = wait_list.get() get_image(photo) count = count + 1 percentage = 100.0*count/total print(&quot;%.1f%% is done on this page %d&quot; %(percentage,page)) id = photo[&apos;id&apos;] visited.append(photo[&apos;id&apos;]) def get_page(page): print(&apos;Dolowding page: &apos;,page) query[&apos;page&apos;] = [str(page),] API = API_BASE + urllib.parse.urlencode(query,doseq=True) ping() print(&apos;Reading list on page %d.&apos; %page) get_list(API) print(&apos;Starting download the images.&apos;) handle_wait_list(page) print(&apos;Page %d is handled successfully.&apos; %page) save_visited() def save_visited(): with open(&apos;visited&apos;,&apos;wb&apos;) as f: pickle.dump(visited,f) def load_visited(): global visited print(&apos;Starting...&apos;) with open(&apos;visited&apos;,&apos;rb&apos;) as f: visited = pickle.load(f) #print(visited) def main(): try: load_visited() except Exception as err: print(err) auth_token = get_token(START_URL) query[&apos;authenticity_token&apos;] = [auth_token,] while True: for page in range(start,end): get_page(page) time.sleep(600) if __name__ == &apos;__main__&apos;: try: main() except KeyboardInterrupt: print(&apos;Exiting, please wait&apos;) save_visited() #except Exception as err: # print(&apos;Somthing is wrong. Exiting...&apos;) # print(err)]]></content>
  </entry>
  <entry>
    <title><![CDATA[大话同步/异步、阻塞/非阻塞]]></title>
    <url>%2Fda-hua-tong-bu-yi-bu-zu-sai-fei-zu-sai%2F</url>
    <content type="text"><![CDATA[原文作者：李博杰 原文地址:https://bojieli.com/2014/11/sync-async-blocked/ (转载请注明出处) 好多人搞不清这两组概念之间的区别。我们拿小明下载文件打个比方。 同步阻塞：小明一直盯着下载进度条，到 100% 的时候就完成。 同步非阻塞：小明提交下载任务后就去干别的，每过一段时间就去瞄一眼进度条，看到 100% 就完成。 异步阻塞：小明换了个有下载完成通知功能的软件，下载完成就“叮”一声。不过小明仍然一直等待“叮”的声音（看起来很傻，不是吗） 异步非阻塞：仍然是那个会“叮”一声的下载软件，小明提交下载任务后就去干别的，听到“叮”的一声就知道完成了。也就是说，同步/异步是下载软件的通知方式，或者说 API 被调用者的通知方式。阻塞/非阻塞则是小明的等待方式，或者说 API 调用者的等待方式。 在不同的场景下，同步/异步、阻塞/非阻塞的四种组合都有应用。 同步阻塞同步阻塞是最简单的方式，就像我们在 C 语言里调用一个函数并等待其返回。 如 stat 系统调用获取文件元数据，只有同步阻塞一种模式。我在访问量很大的一个文件服务器（mirrors.ustc.edu.cn）上遇到过大量 nginx 进程处于 D（uninterruptible）状态的问题，就是因为 stat 系统调用不提供非阻塞 I/O（O_NONBLOCK）选项（nginx 在能用非阻塞 I/O 的地方都用了非阻塞）。文件的元数据被从磁盘中读入进来的时间里，这个 nginx worker 进程只能在内核态苦苦等待而无法做其他事。不提供 O_NONBLOCK 选项，对内核开发者来说这是省事了，但对用户来说就要付出性能的代价了。 同步非阻塞同步非阻塞就是 “每隔一会儿瞄一眼进度条” 的轮询（polling）方式。 ![] (https://bojieli.com/wp-content/uploads/2014/11/99880254f7298bc9d8f9e57a4033584a.png) 同步非阻塞方式相比同步阻塞方式： 优点是能够在等待任务完成的时间里干其他活了（包括提交其他任务，也就是 “后台” 可以有多个任务在同时执行）。 缺点是任务完成的响应延迟增大了，因为每过一段时间才去轮询一次，而任务可能在两次轮询之间的任意时间完成。 由于同步非阻塞方式需要不断轮询，而 “后台” 可能有多个任务在同时进行，人们就想到了循环查询多个任务的完成状态，只要有任何一个任务完成，就去处理它。这就是所谓的 “I/O 多路复用”。UNIX/Linux 下的 select、poll、epoll 就是干这个的（epoll 比 poll、select 效率高，做的事情是一样的）。Windows 下则有 WaitForMultipleObjects 和 IO Completion Ports API 与之对应（Windows API 的命名简直甩 POSIX API 几条街有木有！） Linux I/O 多路复用 高并发的程序一般使用同步非阻塞方式而非多线程 + 同步阻塞方式。要理解这一点，首先要扯到并发和并行的区别。比如去某部门办事需要依次去几个窗口，办事大厅里的人数就是并发数，而窗口个数就是并行度。也就是说并发数是指同时进行的任务数（如同时服务的 HTTP 请求），而并行数是可以同时工作的物理资源数量（如 CPU 核数）。通过合理调度任务的不同阶段，并发数可以远远大于并行度，这就是区区几个 CPU 可以支持上万个用户并发请求的奥秘。在这种高并发的情况下，为每个任务（用户请求）创建一个进程或线程的开销非常大。而同步非阻塞方式可以把多个 I/O 请求丢到后台去，这就可以在一个进程里服务大量的并发 I/O 请求。 异步非阻塞异步非阻塞，就是把一件事丢到 “后台” 去做，完成之后再通知。 在 Linux 中，通知的方式是 “信号”。 如果这个进程正在用户态忙着做别的事（例如在计算两个矩阵的乘积），那就强行打断之，调用事先注册的信号处理函数，这个函数可以决定何时以及如何处理这个异步任务。由于信号处理函数是突然闯进来的，因此跟中断处理程序一样，有很多事情是不能做的，因此保险起见，一般是把事件 “登记” 一下放进队列，然后返回该进程原来在做的事。 如果这个进程正在内核态忙着做别的事，例如以同步阻塞方式读写磁盘，那就只好把这个通知挂起来了，等到内核态的事情忙完了，快要回到用户态的时候，再触发信号通知。 如果这个进程现在被挂起了，例如无事可做 sleep 了，那就把这个进程唤醒，下次有 CPU 空闲的时候，就会调度到这个进程，触发信号通知。异步 API 说来轻巧，做来难，这主要是对 API 的实现者而言的。Linux 的异步 I/O（AIO）支持是 2.6.22 才引入的，还有很多系统调用不支持异步 I/O。Linux 的异步 I/O 最初是为数据库设计的，因此通过异步 I/O 的读写操作不会被缓存或缓冲，这就无法利用操作系统的缓存与缓冲机制。 Windows API 里的异步 I/O API（被称为 Overlapped I/O）则优雅得多，可以在 ReadFileEx、WriteFileEx 等 I/O API 上指定回调函数，当 I/O 操作完成时就会调用它。这相当于在 “信号” 的基础上提供了一层封装。除了指定回调函数，这些异步 I/O 请求还可以使用 “传统” 的同步阻塞方式（WaitForSingleObject）、多路复用的同步非阻塞方式（WaitForMultipleObjects）来等待。多个异步 I/O 请求也可以绑定到一个 I/O Completion Port 上一起等待。 Windows 异步 I/O 原理 很多人把 Linux 的 O_NONBLOCK 认为是异步方式，但事实上这是前面讲的同步非阻塞方式。由于 Linux 的异步 I/O 难用，nginx 早期版本一直使用的是 O_NONBLOCK 和 epoll，从 0.8.11 开始支持异步 I/O，但默认使用的仍然是同步非阻塞方式。需要指出的是，虽然 Linux 上的 I/O API 略显粗糙，但每种编程框架都有封装好的异步 I/O 实现。操作系统少做事，把更多的自由留给用户，正是 UNIX 的设计哲学，也是 Linux 上编程框架百花齐放的一个原因。 异步阻塞都有下载完成通知了，我还傻傻地盯着进度条干什么？这种看起来很傻的方式也是有用的。有时我们的 API 只提供异步通知方式，例如在 node.js 里，但业务逻辑需要的是做完一件事后做另一件事，例如数据库连接初始化后才能开始接受用户的 HTTP 请求。这样的业务逻辑就需要调用者是以阻塞方式来工作。 为了在异步环境里模拟 “顺序执行” 的效果，就需要把同步代码转换成异步形式，这称为 CPS（Continuation Passing Style）变换。BYVoid 大神的 continuation.js 库就是一个 CPS 变换的工具。用户只需用比较符合人类常理的同步方式书写代码，CPS 变换器会把它转换成层层嵌套的异步回调形式。 CPS 变换后的异步代码示例（来源：continuation.js） 用户手写的同步代码示例（来源：continuation.js） 另外一种使用阻塞方式的理由是降低响应延迟。如果采用非阻塞方式，一个任务 A 被提交到后台，就开始做另一件事 B，但 B 还没做完，A 就完成了，这时要想让 A 的完成事件被尽快处理（比如 A 是个紧急事务），要么丢弃做到一半的 B，要么保存 B 的中间状态并切换回 A，任务的切换是需要时间的（不管是从磁盘载入到内存，还是从内存载入到高速缓存），这势必降低 A 的响应速度。因此，对实时系统或者延迟敏感的事务，有时采用阻塞方式比非阻塞方式更好。 最后补充一句，同步/异步的概念在不同语境下是不同的，本文说的是 API 或者 I/O。在其他语境里可能是别的意思，例如分布式系统里的同步表示是各节点按照时钟节拍同步，而异步是收到消息后立即执行。 转载自https://bojieli.com/2014/11/sync-async-blocked/]]></content>
  </entry>
  <entry>
    <title><![CDATA[ghost blog安装过程]]></title>
    <url>%2Fghost-blogan-zhuang-xiang-xi-guo-cheng%2F</url>
    <content type="text"><![CDATA[ghost blog是一个用node.js写成的开源博客平台，由前 WordPress UI 部门主管 John O’Nolan 和 WordPress 高级工程师（女） Hannah Wolfe 创立，目的是为了给用户提供一种更加纯粹的内容写作与发布平台。ghost blog现在有从zip压缩包安装，git安装和npm安装三种方式。其中从zip压缩包安装是最简单也是最快的的方式，本文主要讨论这种安装方式。 从zip安装ghost程序 ghost博客运行在node.js上，首先需要安装Node.js。可以从linux的包管理器安装node.js。在ubuntu系统上，首先运行curl -sL https://deb.nodesource.com/setup | sudo bash -然后运行sudo apt-get install -y nodejs在debian操作系统上（root用户）: apt-get install cur curl -sL https://deb.nodesource.com/setup | bash - 其他操作系统的安装方式见https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager 安装ghost博客。 使用下边的方式下载最新的gost$ curl -L https://ghost.org/zip/ghost-latest.zip -o ghost.zip 解压缩下载的安装包$ unzip -uo ghost.zip -d ghost 在解压缩下载的安装包后，在ghost目录下执行npm install --production 使用下边的命令在developement 环境下启动ghost:npm start 配置反向代理服务器（推荐nginx) 建立文件/etc/nginx/sites-available/ghost.conf.其中添加如下内容： server { listen 80; server_name example.com; location / { proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_pass http://127.0.0.1:2368; } } 将example.com更改成你的域名 在/sites-enabled下创建配置文件的软链接 $ sudo ln -s /etc/nginx/sites-available/ghost.conf /etc/nginx/sites-enabled/ghost.conf 重新载入nginx文件service nigix restart]]></content>
  </entry>
  <entry>
    <title><![CDATA[ubuntu 安装ssh-server并且允许root登陆]]></title>
    <url>%2Fubuntu-an-zhuang-ssh-serverbing-qie-yun-xu-rootdeng-lu%2F</url>
    <content type="text"><![CDATA[安装ssh-server: sudo apt-get install ssh-server 允许root账号通过ssh连接 编辑/etc/ssh/sshd_config文件，找到PermitRootLogin without-password 并在下边添加 PermitRootLogin yes 然后 service ssh restart或service ssh reload]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用Las Vegas随机算法求N皇后问题的解]]></title>
    <url>%2Fshi-yong-las-vegassui-ji-suan-fa-qiu-nhuang-hou-wen-ti-de-jie%2F</url>
    <content type="text"><![CDATA[#include&lt;stdio.h&gt; #include&lt;stdlib.h&gt; #include&lt;time.h&gt; #define bool int #define true 1 #define false 0 #define DIM 50 int uniform(int n){ //random function double r; if(n&lt;=0) return -1; r=rand(); return (int)(r/RAND_MAX*(n-1))+1; } bool QueensLv(int try[]){ //LV算法解决N皇后问题 int index,nb; int col,randcol; bool conflict=false; int row=0; bool success=false; do { //从第一行开始 nb=0; ////计数器，nb值为（row)th皇后的open位置总数 for(col=0;col&lt;DIM;col++){ //从第一列开始 conflict = false; for(index=0;index&lt;row;index++){ if(try[index]==col||abs(col-try[index])==(row-index)) //测试(row,col)是否冲突 conflict=true; } if(conflict==false){ //是安全的 nb=nb+1; if(uniform(nb)==1) //或许放在第col列 randcol=col; //注意第一次uniform一定返回1，即j一定有值 } //endif } //enddo, if(nb &gt;0 ){ //nb=0时无安全位置，第row皇后尚未放好 //在所有nb个安全位置上，(row)th皇后选择位置j的概率为1/nb try[row] = randcol; } row++; }while(nb != 0&amp;&amp;row &lt; DIM); //当前皇后找不到合适的位置或try是 // DIM-promising时结束. if(nb &gt; 0) success = true; else success = false; return success; } int SovleQueen(){ //不断调用LV算法直至成功 bool success; int chess[DIM]; int row; int i; for(i=0;i&lt;DIM;i++)chess[i]=-1; do{ success = QueensLv(chess); }while(success!=true); for(row = 0;row&lt;DIM;row++){ //printf(&quot;%d &quot;,chess[row]); for(i = 0; i &lt; chess[row]; i++)printf(&quot;.&quot;); printf(&quot;X&quot;); for(i=chess[row]+1;i&lt;DIM;i++)printf(&quot;.&quot;); printf(&quot;\n&quot;); } } int main(int argc,char **argv){ int index,j; clock_t start,end; double dur; srand(time(0)); start = clock(); SovleQueen(); end = clock(); dur = 1000.0*(end-start)/CLOCKS_PER_SEC; printf(&quot;used: %f ms.\n&quot;,dur); }]]></content>
      <tags>
        <tag>
- 随机算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电信服务商屏蔽digitalocean新加坡ip]]></title>
    <url>%2Fdian-xin-fu-wu-shang-ping-bi-digitaloceanxin-jia-po-ip%2F</url>
    <content type="text"><![CDATA[因为最近在旧金山的digitalocean上的vps访问起来ping值越来越高，平均都在200以上了，所以今天想把droplet转移到新加坡机房。转移过后问题出现了。丢包特别严重平均50%。而且到后来ssh都无法连接成功。 给support提交了ticket，他们让我们traceroute和ping ，把结果给他们看。最后客服给回复后，他Ping我的Ip也是很高的丢包，但是给其他任何地方ping都不会产生如此高的丢包。 室友说是GFW的问题。我觉得很认同。 晚上的时候，我突然想到换个网络通出口，我平时一直用的是电信出口。我将网络通出口换成了移动出口，这一次，ssh连接成功了。延迟很低。就像在本地一样。但是如果用移动出口，访问国内网站又会效果不好。我这一次换成了教育网出口。显而易见，也是非常的通畅。把shadowsocks本地的配置更改了一下，浏览器代理加了规则list:http://autoproxy-gfwlist.googlecode.com/svn/trunk/gfwlist.txt。 今天完工。 该死的电信防火墙。]]></content>
      <tags>
        <tag>
- vps
- digitalocean</tag>
      </tags>
  </entry>
</search>
